"""\nSnowflake ML Workflow Template with Experiment Tracking\n========================================================\nEnd-to-end ML pipeline for classification tasks with:\n- Feature selection using Pearson correlation\n- Feature Store integration\n- Snowflake Experiments tracking for model comparison\n- AutoGluon AutoML training\n- Model evaluation and selection\n- Model Registry (Warehouse + Container Services)\n\nAuthor: Randal Scott King\n"""\n\nimport snowflake.snowpark as snowpark\nfrom snowflake.snowpark.functions import col\nfrom snowflake.ml.feature_store import (\n    FeatureStore,\n    FeatureView,\n    Entity,\n    CreationMode\n)\nfrom snowflake.ml.registry import Registry\nfrom snowflake.ml.experiment import ExperimentTracking\nfrom snowflake.ml.modeling.preprocessing import StandardScaler\nfrom snowflake.snowpark import Session\nimport pandas as pd\nimport numpy as np\nfrom autogluon.tabular import TabularPredictor\nfrom typing import List, Dict, Tuple, Optional\nimport json\nfrom datetime import datetime\n\n\nclass SnowflakeMLWorkflow:\n    \"\"\"\n    Comprehensive ML workflow for Snowflake with feature engineering,\n    AutoGluon training, experiment tracking, and model registry integration.\n    \"\"\"\n    \n    def __init__(\n        self,\n        session: Session,\n        source_table: str,\n        target_column: str,\n        feature_store_db: str = \"ML_FEATURES\",\n        model_registry_db: str = \"ML_MODELS\",\n        correlation_threshold: float = 0.95,\n        experiment_name: Optional[str] = None\n    ):\n        \"\"\"\n        Initialize the ML workflow.\n        \n        Args:\n            session: Snowflake Snowpark session\n            source_table: Fully qualified table name (DB.SCHEMA.TABLE)\n            target_column: Name of the target variable column\n            feature_store_db: Database for feature store\n            model_registry_db: Database for model registry\n            correlation_threshold: Threshold for dropping highly correlated features\n            experiment_name: Name for experiment tracking (optional)\n        \"\"\"\n        self.session = session\n        self.source_table = source_table\n        self.target_column = target_column\n        self.feature_store_db = feature_store_db\n        self.model_registry_db = model_registry_db\n        self.correlation_threshold = correlation_threshold\n        \n        # Initialize Feature Store and Registry\n        self.feature_store = FeatureStore(\n            session=session,\n            database=feature_store_db,\n            name=\"ML_FEATURE_STORE\",\n            default_warehouse=session.get_current_warehouse(),\n            creation_mode=CreationMode.CREATE_IF_NOT_EXIST\n        )\n        \n        self.registry = Registry(\n            session=session,\n            database_name=model_registry_db,\n            schema_name=\"MODEL_REGISTRY\"\n        )\n        \n        # Initialize Experiment Tracking\n        self.exp = ExperimentTracking(session=session)\n        if experiment_name:\n            self.exp.set_experiment(experiment_name)\n            print(f\"✓ Experiment '{experiment_name}' initialized\")\n        \n    def load_data(self) -> snowpark.DataFrame:\n        \"\"\"Load data from source table.\"\"\"\n        print(f\"Loading data from {self.source_table}...\")\n        df = self.session.table(self.source_table)\n        row_count = df.count()\n        print(f\"✓ Loaded {row_count:,} rows\")\n        return df\n    \n    def select_features_by_correlation(\n        self,\n        df: snowpark.DataFrame,\n        exclude_columns: Optional[List[str]] = None\n    ) -> Tuple[List[str], Dict]:\n        \"\"\"\n        Select features by removing highly correlated columns.\n        \n        Uses Pearson correlation to identify and remove redundant features.\n        Keeps the first feature in each highly correlated pair.\n        \n        Args:\n            df: Snowpark DataFrame\n            exclude_columns: Columns to exclude from analysis (e.g., IDs, target)\n            \n        Returns:\n            Tuple of (selected_features, correlation_report)\n        \"\"\"\n        print(\"Calculating feature correlations...\")\n        \n        # Get numeric columns only\n        pdf = df.to_pandas()\n        numeric_cols = pdf.select_dtypes(include=[np.number]).columns.tolist()\n        \n        # Exclude specified columns\n        if exclude_columns is None:\n            exclude_columns = [self.target_column]\n        else:\n            exclude_columns = list(set(exclude_columns + [self.target_column]))\n        \n        feature_cols = [col for col in numeric_cols if col not in exclude_columns]\n        \n        # Calculate correlation matrix\n        corr_matrix = pdf[feature_cols].corr().abs()\n        \n        # Find features to drop\n        upper_triangle = np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)\n        upper_corr = corr_matrix.where(upper_triangle)\n        \n        to_drop = []\n        dropped_pairs = []\n        \n        for column in upper_corr.columns:\n            correlated = upper_corr[column][upper_corr[column] > self.correlation_threshold]\n            if not correlated.empty:\n                for idx, corr_value in correlated.items():\n                    if column not in to_drop:\n                        to_drop.append(column)\n                        dropped_pairs.append({\n                            'kept_feature': idx,\n                            'dropped_feature': column,\n                            'correlation': float(corr_value)\n                        })\n        \n        selected_features = [col for col in feature_cols if col not in to_drop]\n        \n        report = {\n            'total_numeric_features': len(feature_cols),\n            'selected_features': len(selected_features),\n            'dropped_features': len(to_drop),\n            'correlation_threshold': self.correlation_threshold,\n            'dropped_pairs': dropped_pairs,\n            'selected_feature_list': selected_features\n        }\n        \n        print(f\"✓ Feature selection complete:\")\n        print(f\"  - Original features: {len(feature_cols)}\")\n        print(f\"  - Selected features: {len(selected_features)}\")\n        print(f\"  - Dropped features: {len(to_drop)}\")\n        \n        return selected_features, report\n    \n    def create_feature_view(\n        self,\n        df: snowpark.DataFrame,\n        selected_features: List[str],\n        entity_column: str,\n        feature_view_name: str\n    ) -> FeatureView:\n        \"\"\"\n        Create and register a Feature View in the Feature Store.\n        \n        Args:\n            df: Snowpark DataFrame with features\n            selected_features: List of feature column names\n            entity_column: Column to use as entity (e.g., customer_id)\n            feature_view_name: Name for the feature view\n            \n        Returns:\n            FeatureView object\n        \"\"\"\n        print(f\"Creating feature view '{feature_view_name}'...\")\n        \n        # Define entity\n        entity = Entity(name=entity_column, join_keys=[entity_column])\n        \n        # Select features and entity\n        feature_cols = selected_features + [entity_column, self.target_column]\n        feature_df = df.select(feature_cols)\n        \n        # Create feature view\n        feature_view = FeatureView(\n            name=feature_view_name,\n            entities=[entity],\n            feature_df=feature_df,\n            refresh_freq=\"1 day\",\n            desc=f\"Curated features for ML model training - Created {datetime.now()}\"\n        )\n        \n        # Register in feature store\n        self.feature_store.register_feature_view(\n            feature_view=feature_view,\n            version=\"1.0\",\n            block=True\n        )\n        \n        print(f\"✓ Feature view '{feature_view_name}' registered successfully\")\n        return feature_view\n    \n    def load_features_for_training(\n        self,\n        feature_view_name: str,\n        version: str = \"1.0\"\n    ) -> pd.DataFrame:\n        \"\"\"\n        Load features from Feature Store for model training.\n        \n        Args:\n            feature_view_name: Name of the feature view\n            version: Version of the feature view\n            \n        Returns:\n            Pandas DataFrame with features\n        \"\"\"\n        print(f\"Loading features from Feature Store: {feature_view_name} v{version}\")\n        \n        # Load as pandas DataFrame\n        spine_df = self.session.sql(f\"\"\"\n            SELECT * FROM {self.feature_store_db}.{feature_view_name}\n        \"\"\")\n        \n        pdf = spine_df.to_pandas()\n        print(f\"✓ Loaded {len(pdf):,} rows with {len(pdf.columns)} columns\")\n        \n        return pdf\n    \n    def train_autogluon_models(\n        self,\n        train_data: pd.DataFrame,\n        time_limit: int = 3600,\n        presets: str = \"best_quality\",\n        num_bag_folds: int = 5,\n        num_stack_levels: int = 1,\n        eval_metric: str = \"f1\"\n    ) -> TabularPredictor:\n        \"\"\"\n        Train multiple classification models using AutoGluon.\n        \n        Args:\n            train_data: Training data with target column\n            time_limit: Time limit in seconds for training\n            presets: AutoGluon preset ('best_quality', 'high_quality', 'medium_quality')\n            num_bag_folds: Number of folds for bagging\n            num_stack_levels: Number of stacking levels\n            eval_metric: Evaluation metric for model selection\n            \n        Returns:\n            Trained TabularPredictor\n        \"\"\"\n        print(\"Starting AutoGluon training...\")\n        print(f\"  - Time limit: {time_limit}s ({time_limit/60:.1f} minutes)\")\n        print(f\"  - Presets: {presets}\")\n        print(f\"  - Bag folds: {num_bag_folds}\")\n        print(f\"  - Stack levels: {num_stack_levels}\")\n        \n        # Configure AutoGluon\n        predictor = TabularPredictor(\n            label=self.target_column,\n            problem_type='binary',  # Change to 'multiclass' if needed\n            eval_metric=eval_metric,\n            path='./autogluon_models'\n        )\n        \n        # Train models\n        predictor.fit(\n            train_data=train_data,\n            time_limit=time_limit,\n            presets=presets,\n            num_bag_folds=num_bag_folds,\n            num_stack_levels=num_stack_levels,\n            verbosity=2\n        )\n        \n        print(\"✓ AutoGluon training complete!\")\n        return predictor\n    \n    def evaluate_and_select_best_model(\n        self,\n        predictor: TabularPredictor,\n        test_data: pd.DataFrame\n    ) -> Dict:\n        \"\"\"\n        Evaluate all trained models and select the best one.\n        \n        Selection criteria (in priority order):\n        1. F1 Score\n        2. Recall\n        3. Precision\n        \n        Args:\n            predictor: Trained AutoGluon predictor\n            test_data: Test data for evaluation\n            \n        Returns:\n            Dictionary with best model info and all model metrics\n        \"\"\"\n        print(\"Evaluating models...\")\n        \n        # Get leaderboard with detailed metrics\n        leaderboard = predictor.leaderboard(data=test_data, silent=True)\n        \n        # Get detailed metrics for each model\n        model_metrics = []\n        \n        for model_name in leaderboard['model']:\n            try:\n                # Get predictions\n                y_true = test_data[self.target_column]\n                y_pred = predictor.predict(test_data, model=model_name)\n                \n                # Calculate metrics\n                from sklearn.metrics import f1_score, recall_score, precision_score, accuracy_score\n                \n                f1 = f1_score(y_true, y_pred, average='weighted')\n                recall = recall_score(y_true, y_pred, average='weighted')\n                precision = precision_score(y_true, y_pred, average='weighted')\n                accuracy = accuracy_score(y_true, y_pred)\n                \n                model_metrics.append({\n                    'model_name': model_name,\n                    'f1_score': f1,\n                    'recall': recall,\n                    'precision': precision,\n                    'accuracy': accuracy,\n                    'score_val': leaderboard[leaderboard['model'] == model_name]['score_val'].values[0]\n                })\n            except Exception as e:\n                print(f\"Warning: Could not evaluate {model_name}: {e}\")\n        \n        # Convert to DataFrame for easier analysis\n        metrics_df = pd.DataFrame(model_metrics)\n        \n        # Select best model based on F1, then Recall, then Precision\n        metrics_df = metrics_df.sort_values(\n            by=['f1_score', 'recall', 'precision'],\n            ascending=False\n        )\n        \n        best_model = metrics_df.iloc[0].to_dict()\n        \n        print(f\"\\n{'='*60}\")\n        print(f\"BEST MODEL SELECTED: {best_model['model_name']}\")\n        print(f\"{'='*60}\")\n        print(f\"F1 Score:  {best_model['f1_score']:.4f}\")\n        print(f\"Recall:    {best_model['recall']:.4f}\")\n        print(f\"Precision: {best_model['precision']:.4f}\")\n        print(f\"Accuracy:  {best_model['accuracy']:.4f}\")\n        print(f\"{'='*60}\")\n        \n        return {\n            'best_model': best_model,\n            'all_models': metrics_df.to_dict('records'),\n            'leaderboard': leaderboard\n        }\n    \n    def log_to_experiment(\n        self,\n        run_name: str,\n        params: Dict,\n        metrics: Dict,\n        predictor: Optional[TabularPredictor] = None,\n        model_name: Optional[str] = None\n    ):\n        \"\"\"\n        Log parameters, metrics, and model to Snowflake Experiments.\n        \n        Args:\n            run_name: Name for the experiment run\n            params: Dictionary of parameters to log\n            metrics: Dictionary of metrics to log\n            predictor: Optional trained model to log\n            model_name: Optional name for the logged model\n        \"\"\"\n        print(f\"Logging to experiment run '{run_name}'...\")\n        \n        with self.exp.start_run(run_name):\n            # Log parameters\n            self.exp.log_params(params)\n            \n            # Log metrics\n            self.exp.log_metrics(metrics)\n            \n            # Log model if provided\n            if predictor and model_name:\n                self.exp.log_model(\n                    model=predictor,\n                    model_name=model_name\n                )\n            \n            print(f\"✓ Logged to experiment run '{run_name}'\")\n    \n    def register_model_to_warehouse(\n        self,\n        predictor: TabularPredictor,\n        model_name: str,\n        best_model_name: str,\n        metrics: Dict,\n        feature_list: List[str]\n    ) -> str:\n        \"\"\"\n        Register the best model to Snowflake Model Registry (Warehouse).\n        \n        Args:\n            predictor: Trained AutoGluon predictor\n            model_name: Name for the registered model\n            best_model_name: Name of the best performing model\n            metrics: Model evaluation metrics\n            feature_list: List of features used\n            \n        Returns:\n            Model version string\n        \"\"\"\n        print(f\"Registering model '{model_name}' to Snowflake Model Registry...\")\n        \n        # Prepare model metadata\n        metadata = {\n            'model_type': 'AutoGluon TabularPredictor',\n            'best_model': best_model_name,\n            'f1_score': metrics['best_model']['f1_score'],\n            'recall': metrics['best_model']['recall'],\n            'precision': metrics['best_model']['precision'],\n            'accuracy': metrics['best_model']['accuracy'],\n            'features': feature_list,\n            'target_column': self.target_column,\n            'training_timestamp': datetime.now().isoformat()\n        }\n        \n        # Register model\n        model_version = self.registry.log_model(\n            model_name=model_name,\n            model_version='v1',\n            model=predictor,\n            conda_dependencies=['autogluon.tabular'],\n            metadata=metadata,\n            comment=f\"Best model: {best_model_name} with F1={metrics['best_model']['f1_score']:.4f}\"\n        )\n        \n        print(f\"✓ Model registered successfully: {model_version}\")\n        return model_version\n    \n    def deploy_to_container_services(\n        self,\n        model_name: str,\n        model_version: str,\n        compute_pool: str,\n        service_name: str\n    ) -> str:\n        \"\"\"\n        Deploy model to Snowflake Container Services.\n        \n        Args:\n            model_name: Registered model name\n            model_version: Model version\n            compute_pool: Snowflake compute pool for the service\n            service_name: Name for the deployed service\n            \n        Returns:\n            Service endpoint URL\n        \"\"\"\n        print(f\"Deploying model to Container Services: {service_name}\")\n        \n        # Create service specification\n        service_spec = f\"\"\"\n        CREATE SERVICE IF NOT EXISTS {service_name}\n        IN COMPUTE POOL {compute_pool}\n        FROM @ML_MODELS.MODEL_REGISTRY.{model_name}/{model_version}\n        SPECIFICATION = $$\n        spec:\n          containers:\n          - name: model-inference\n            image: /ml_models/model_registry/autogluon:latest\n            env:\n              MODEL_NAME: {model_name}\n              MODEL_VERSION: {model_version}\n          endpoints:\n          - name: predict\n            port: 8080\n            protocol: http\n        $$\n        \"\"\"\n        \n        # Execute service creation\n        self.session.sql(service_spec).collect()\n        \n        endpoint = f\"https://{service_name}.snowflakecomputing.app/predict\"\n        \n        print(f\"✓ Service deployed successfully!\")\n        print(f\"✓ Endpoint: {endpoint}\")\n        \n        return endpoint\n    \n    def run_complete_workflow(\n        self,\n        entity_column: str,\n        feature_view_name: str,\n        model_name: str,\n        test_size: float = 0.2,\n        random_state: int = 42,\n        time_limit: int = 3600,\n        presets: str = \"best_quality\",\n        num_bag_folds: int = 5,\n        num_stack_levels: int = 1,\n        eval_metric: str = \"f1\",\n        deploy_to_container: bool = False,\n        compute_pool: Optional[str] = None,\n        service_name: Optional[str] = None,\n        experiment_run_name: Optional[str] = None\n    ) -> Dict:\n        \"\"\"\n        Execute the complete ML workflow end-to-end with experiment tracking.\n        \n        Args:\n            entity_column: Column to use as entity identifier\n            feature_view_name: Name for the feature view\n            model_name: Name for the registered model\n            test_size: Proportion of data for testing\n            random_state: Random seed for reproducibility\n            time_limit: Time limit for AutoGluon training\n            presets: AutoGluon presets\n            num_bag_folds: Number of bagging folds\n            num_stack_levels: Number of stacking levels\n            eval_metric: Evaluation metric\n            deploy_to_container: Whether to deploy to Container Services\n            compute_pool: Compute pool for container deployment\n            service_name: Service name for container deployment\n            experiment_run_name: Name for experiment run\n            \n        Returns:\n            Dictionary with workflow results\n        \"\"\"\n        print(\"=\"*80)\n        print(\"STARTING COMPLETE ML WORKFLOW WITH EXPERIMENT TRACKING\")\n        print(\"=\"*80)\n        \n        # Step 1: Load data\n        df = self.load_data()\n        \n        # Step 2: Feature selection\n        selected_features, correlation_report = self.select_features_by_correlation(df)\n        \n        # Step 3: Create feature view\n        feature_view = self.create_feature_view(\n            df=df,\n            selected_features=selected_features,\n            entity_column=entity_column,\n            feature_view_name=feature_view_name\n        )\n        \n        # Step 4: Load features for training\n        training_data = self.load_features_for_training(feature_view_name)\n        \n        # Step 5: Split data\n        from sklearn.model_selection import train_test_split\n        train_df, test_df = train_test_split(\n            training_data,\n            test_size=test_size,\n            random_state=random_state,\n            stratify=training_data[self.target_column]\n        )\n        \n        print(f\"✓ Data split: {len(train_df):,} train, {len(test_df):,} test\")\n        \n        # Prepare experiment parameters\n        exp_params = {\n            'source_table': self.source_table,\n            'target_column': self.target_column,\n            'correlation_threshold': self.correlation_threshold,\n            'test_size': test_size,\n            'random_state': random_state,\n            'time_limit': time_limit,\n            'presets': presets,\n            'num_bag_folds': num_bag_folds,\n            'num_stack_levels': num_stack_levels,\n            'eval_metric': eval_metric,\n            'total_features': correlation_report['total_numeric_features'],\n            'selected_features': correlation_report['selected_features'],\n            'dropped_features': correlation_report['dropped_features']\n        }\n        \n        # Step 6: Train AutoGluon models\n        predictor = self.train_autogluon_models(\n            train_data=train_df,\n            time_limit=time_limit,\n            presets=presets,\n            num_bag_folds=num_bag_folds,\n            num_stack_levels=num_stack_levels,\n            eval_metric=eval_metric\n        )\n        \n        # Step 7: Evaluate and select best model\n        evaluation_results = self.evaluate_and_select_best_model(\n            predictor=predictor,\n            test_data=test_df\n        )\n        \n        # Prepare experiment metrics\n        exp_metrics = {\n            'f1_score': evaluation_results['best_model']['f1_score'],\n            'recall': evaluation_results['best_model']['recall'],\n            'precision': evaluation_results['best_model']['precision'],\n            'accuracy': evaluation_results['best_model']['accuracy'],\n            'models_trained': len(evaluation_results['all_models'])\n        }\n        \n        # Step 8: Log to experiment\n        if experiment_run_name:\n            self.log_to_experiment(\n                run_name=experiment_run_name,\n                params=exp_params,\n                metrics=exp_metrics,\n                predictor=predictor,\n                model_name=model_name\n            )\n        \n        # Step 9: Register to Model Registry\n        model_version = self.register_model_to_warehouse(\n            predictor=predictor,\n            model_name=model_name,\n            best_model_name=evaluation_results['best_model']['model_name'],\n            metrics=evaluation_results,\n            feature_list=selected_features\n        )\n        \n        # Step 10: Optional - Deploy to Container Services\n        service_endpoint = None\n        if deploy_to_container and compute_pool and service_name:\n            service_endpoint = self.deploy_to_container_services(\n                model_name=model_name,\n                model_version=model_version,\n                compute_pool=compute_pool,\n                service_name=service_name\n            )\n        \n        # Compile results\n        results = {\n            'workflow_status': 'SUCCESS',\n            'timestamp': datetime.now().isoformat(),\n            'data_info': {\n                'source_table': self.source_table,\n                'total_rows': len(training_data),\n                'train_rows': len(train_df),\n                'test_rows': len(test_df)\n            },\n            'feature_selection': correlation_report,\n            'feature_view': feature_view_name,\n            'model_info': {\n                'model_name': model_name,\n                'model_version': model_version,\n                'best_model': evaluation_results['best_model']['model_name']\n            },\n            'performance_metrics': evaluation_results['best_model'],\n            'all_models_evaluated': len(evaluation_results['all_models']),\n            'service_endpoint': service_endpoint,\n            'experiment_run': experiment_run_name\n        }\n        \n        print(\"=\"*80)\n        print(\"WORKFLOW COMPLETED SUCCESSFULLY\")\n        print(\"=\"*80)\n        print(json.dumps(results, indent=2, default=str))\n        \n        return results\n