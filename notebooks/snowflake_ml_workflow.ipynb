{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Snowflake ML Workflow with AutoGluon\n",
    "\n",
    "## End-to-End ML Pipeline for Classification\n",
    "\n",
    "**Author:** Randal Scott King  \n",
    "**Date:** December 2024\n",
    "\n",
    "This notebook demonstrates a complete ML workflow in Snowflake including:\n",
    "- Automated feature selection using Pearson correlation\n",
    "- Feature Store integration with versioning\n",
    "- AutoGluon AutoML training (LightGBM, XGBoost, CatBoost, RF, Neural Networks)\n",
    "- Model evaluation and selection (F1, Recall, Precision)\n",
    "- Model Registry for versioning and lineage\n",
    "- Optional deployment to Snowflake Container Services"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n",
    "\n",
    "Import all necessary libraries for the ML workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Snowflake libraries\n",
    "import snowflake.snowpark as snowpark\n",
    "from snowflake.snowpark.functions import col\n",
    "from snowflake.ml.feature_store import (\n",
    "    FeatureStore,\n",
    "    FeatureView,\n",
    "    Entity,\n",
    "    CreationMode\n",
    ")\n",
    "from snowflake.ml.registry import Registry\n",
    "from snowflake.ml.modeling.preprocessing import StandardScaler\n",
    "from snowflake.snowpark import Session\n",
    "\n",
    "# Import ML libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from autogluon.tabular import TabularPredictor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score, accuracy_score\n",
    "\n",
    "# Import utilities\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"✓ All libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "Set up your configuration parameters for the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Workflow Configuration\n",
    "CONFIG = {\n",
    "    # Data Configuration\n",
    "    'source_table': 'ML_DATA.PUBLIC.CUSTOMER_FEATURES',\n",
    "    'target_column': 'CHURN_FLAG',\n",
    "    'entity_column': 'CUSTOMER_ID',\n",
    "    \n",
    "    # Feature Engineering\n",
    "    'correlation_threshold': 0.95,\n",
    "    'feature_store_db': 'ML_FEATURES',\n",
    "    'feature_view_name': 'customer_churn_features',\n",
    "    \n",
    "    # Model Training\n",
    "    'model_registry_db': 'ML_MODELS',\n",
    "    'model_name': 'customer_churn_classifier',\n",
    "    'test_size': 0.2,\n",
    "    'random_state': 42,\n",
    "    \n",
    "    # AutoGluon Settings\n",
    "    'time_limit': 3600,  # 1 hour\n",
    "    'presets': 'best_quality',\n",
    "    'num_bag_folds': 5,\n",
    "    'num_stack_levels': 1,\n",
    "    'eval_metric': 'f1',\n",
    "    \n",
    "    # Container Services (optional)\n",
    "    'deploy_to_container': False,\n",
    "    'compute_pool': 'ML_COMPUTE_POOL',\n",
    "    'service_name': 'churn_prediction_service'\n",
    "}\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(json.dumps(CONFIG, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initialize Snowflake Session\n",
    "\n",
    "Get the current Snowflake session (automatically available in Snowflake notebooks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Snowflake notebooks, session is automatically available\n",
    "# If running locally, uncomment and configure:\n",
    "# session = Session.builder.configs({\n",
    "#     \"account\": \"your_account\",\n",
    "#     \"user\": \"your_user\",\n",
    "#     \"password\": \"your_password\",\n",
    "#     \"role\": \"ML_ENGINEER\",\n",
    "#     \"warehouse\": \"ML_WAREHOUSE\",\n",
    "#     \"database\": \"ML_DATA\",\n",
    "#     \"schema\": \"PUBLIC\"\n",
    "# }).create()\n",
    "\n",
    "# Use the existing session in Snowflake notebook\n",
    "print(f\"Connected to Snowflake\")\n",
    "print(f\"Current role: {session.get_current_role()}\")\n",
    "print(f\"Current warehouse: {session.get_current_warehouse()}\")\n",
    "print(f\"Current database: {session.get_current_database()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Initialize Feature Store and Model Registry\n",
    "\n",
    "Set up connections to Snowflake's Feature Store and Model Registry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Feature Store\n",
    "feature_store = FeatureStore(\n",
    "    session=session,\n",
    "    database=CONFIG['feature_store_db'],\n",
    "    name=\"ML_FEATURE_STORE\",\n",
    "    default_warehouse=session.get_current_warehouse(),\n",
    "    creation_mode=CreationMode.CREATE_IF_NOT_EXIST\n",
    ")\n",
    "\n",
    "print(\"✓ Feature Store initialized\")\n",
    "\n",
    "# Initialize Model Registry\n",
    "registry = Registry(\n",
    "    session=session,\n",
    "    database_name=CONFIG['model_registry_db'],\n",
    "    schema_name=\"MODEL_REGISTRY\"\n",
    ")\n",
    "\n",
    "print(\"✓ Model Registry initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load Data\n",
    "\n",
    "Load the source data from Snowflake table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from source table\n",
    "print(f\"Loading data from {CONFIG['source_table']}...\")\n",
    "df = session.table(CONFIG['source_table'])\n",
    "\n",
    "# Display data info\n",
    "row_count = df.count()\n",
    "columns = df.columns\n",
    "\n",
    "print(f\"✓ Loaded {row_count:,} rows\")\n",
    "print(f\"✓ Columns: {len(columns)}\")\n",
    "print(f\"\\nColumn names: {', '.join(columns[:10])}...\")\n",
    "\n",
    "# Show sample data\n",
    "df.limit(5).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feature Selection by Correlation\n",
    "\n",
    "Automatically select features by removing highly correlated pairs using Pearson correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_features_by_correlation(\n",
    "    df: snowpark.DataFrame,\n",
    "    target_column: str,\n",
    "    threshold: float = 0.95\n",
    ") -> Tuple[List[str], Dict]:\n",
    "    \"\"\"\n",
    "    Select features by removing highly correlated columns.\n",
    "    \"\"\"\n",
    "    print(\"Calculating feature correlations...\")\n",
    "    \n",
    "    # Convert to pandas for correlation analysis\n",
    "    pdf = df.to_pandas()\n",
    "    numeric_cols = pdf.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    \n",
    "    # Exclude target column\n",
    "    feature_cols = [col for col in numeric_cols if col != target_column]\n",
    "    \n",
    "    # Calculate correlation matrix\n",
    "    corr_matrix = pdf[feature_cols].corr().abs()\n",
    "    \n",
    "    # Find features to drop\n",
    "    upper_triangle = np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)\n",
    "    upper_corr = corr_matrix.where(upper_triangle)\n",
    "    \n",
    "    to_drop = []\n",
    "    dropped_pairs = []\n",
    "    \n",
    "    for column in upper_corr.columns:\n",
    "        correlated = upper_corr[column][upper_corr[column] > threshold]\n",
    "        if not correlated.empty:\n",
    "            for idx, corr_value in correlated.items():\n",
    "                if column not in to_drop:\n",
    "                    to_drop.append(column)\n",
    "                    dropped_pairs.append({\n",
    "                        'kept_feature': idx,\n",
    "                        'dropped_feature': column,\n",
    "                        'correlation': float(corr_value)\n",
    "                    })\n",
    "    \n",
    "    selected_features = [col for col in feature_cols if col not in to_drop]\n",
    "    \n",
    "    report = {\n",
    "        'total_numeric_features': len(feature_cols),\n",
    "        'selected_features': len(selected_features),\n",
    "        'dropped_features': len(to_drop),\n",
    "        'correlation_threshold': threshold,\n",
    "        'dropped_pairs': dropped_pairs,\n",
    "        'selected_feature_list': selected_features\n",
    "    }\n",
    "    \n",
    "    return selected_features, report\n",
    "\n",
    "# Execute feature selection\n",
    "selected_features, correlation_report = select_features_by_correlation(\n",
    "    df=df,\n",
    "    target_column=CONFIG['target_column'],\n",
    "    threshold=CONFIG['correlation_threshold']\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Feature selection complete:\")\n",
    "print(f\"  - Original features: {correlation_report['total_numeric_features']}\")\n",
    "print(f\"  - Selected features: {correlation_report['selected_features']}\")\n",
    "print(f\"  - Dropped features: {correlation_report['dropped_features']}\")\n",
    "\n",
    "# Show dropped pairs\n",
    "if correlation_report['dropped_pairs']:\n",
    "    print(f\"\\nDropped feature pairs (showing first 5):\")\n",
    "    for pair in correlation_report['dropped_pairs'][:5]:\n",
    "        print(f\"  - Kept: {pair['kept_feature']}, Dropped: {pair['dropped_feature']}, Corr: {pair['correlation']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Create Feature View in Feature Store\n",
    "\n",
    "Register selected features in Snowflake Feature Store for versioning and reuse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_feature_view(\n",
    "    df: snowpark.DataFrame,\n",
    "    selected_features: List[str],\n",
    "    entity_column: str,\n",
    "    target_column: str,\n",
    "    feature_view_name: str\n",
    ") -> FeatureView:\n",
    "    \"\"\"\n",
    "    Create and register a Feature View in the Feature Store.\n",
    "    \"\"\"\n",
    "    print(f\"Creating feature view '{feature_view_name}'...\")\n",
    "    \n",
    "    # Define entity\n",
    "    entity = Entity(name=entity_column, join_keys=[entity_column])\n",
    "    \n",
    "    # Select features and entity\n",
    "    feature_cols = selected_features + [entity_column, target_column]\n",
    "    feature_df = df.select(feature_cols)\n",
    "    \n",
    "    # Create feature view\n",
    "    feature_view = FeatureView(\n",
    "        name=feature_view_name,\n",
    "        entities=[entity],\n",
    "        feature_df=feature_df,\n",
    "        refresh_freq=\"1 day\",\n",
    "        desc=f\"Curated features for ML model training - Created {datetime.now()}\"\n",
    "    )\n",
    "    \n",
    "    # Register in feature store\n",
    "    feature_store.register_feature_view(\n",
    "        feature_view=feature_view,\n",
    "        version=\"1.0\",\n",
    "        block=True\n",
    "    )\n",
    "    \n",
    "    return feature_view\n",
    "\n",
    "# Create feature view\n",
    "feature_view = create_feature_view(\n",
    "    df=df,\n",
    "    selected_features=selected_features,\n",
    "    entity_column=CONFIG['entity_column'],\n",
    "    target_column=CONFIG['target_column'],\n",
    "    feature_view_name=CONFIG['feature_view_name']\n",
    ")\n",
    "\n",
    "print(f\"✓ Feature view '{CONFIG['feature_view_name']}' registered successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Load Features for Training\n",
    "\n",
    "Load the features from Feature Store and prepare for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_features_for_training(\n",
    "    feature_view_name: str,\n",
    "    version: str = \"1.0\"\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load features from Feature Store for model training.\n",
    "    \"\"\"\n",
    "    print(f\"Loading features from Feature Store: {feature_view_name} v{version}\")\n",
    "    \n",
    "    # Load as pandas DataFrame\n",
    "    spine_df = session.sql(f\"\"\"\n",
    "        SELECT * FROM {CONFIG['feature_store_db']}.{feature_view_name}\n",
    "    \"\"\")\n",
    "    \n",
    "    pdf = spine_df.to_pandas()\n",
    "    print(f\"✓ Loaded {len(pdf)} rows with {len(pdf.columns)} columns\")\n",
    "    \n",
    "    return pdf\n",
    "\n",
    "# Load training data\n",
    "training_data = load_features_for_training(\n",
    "    feature_view_name=CONFIG['feature_view_name']\n",
    ")\n",
    "\n",
    "# Display data info\n",
    "print(f\"\\nData shape: {training_data.shape}\")\n",
    "print(f\"Target distribution:\")\n",
    "print(training_data[CONFIG['target_column']].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Split Data into Train and Test Sets\n",
    "\n",
    "Create stratified train/test split for model training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "train_df, test_df = train_test_split(\n",
    "    training_data,\n",
    "    test_size=CONFIG['test_size'],\n",
    "    random_state=CONFIG['random_state'],\n",
    "    stratify=training_data[CONFIG['target_column']]\n",
    ")\n",
    "\n",
    "print(f\"✓ Data split complete:\")\n",
    "print(f\"  - Training set: {len(train_df):,} rows ({len(train_df)/len(training_data)*100:.1f}%)\")\n",
    "print(f\"  - Test set: {len(test_df):,} rows ({len(test_df)/len(training_data)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nTraining set target distribution:\")\n",
    "print(train_df[CONFIG['target_column']].value_counts())\n",
    "\n",
    "print(f\"\\nTest set target distribution:\")\n",
    "print(test_df[CONFIG['target_column']].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Train Models with AutoGluon\n",
    "\n",
    "Train multiple classification models using AutoGluon AutoML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_autogluon_models(\n",
    "    train_data: pd.DataFrame,\n",
    "    target_column: str,\n",
    "    time_limit: int = 3600,\n",
    "    presets: str = \"best_quality\",\n",
    "    eval_metric: str = \"f1\",\n",
    "    num_bag_folds: int = 5,\n",
    "    num_stack_levels: int = 1\n",
    ") -> TabularPredictor:\n",
    "    \"\"\"\n",
    "    Train multiple classification models using AutoGluon.\n",
    "    \"\"\"\n",
    "    print(\"Starting AutoGluon training...\")\n",
    "    print(f\"  - Time limit: {time_limit}s ({time_limit/60:.1f} minutes)\")\n",
    "    print(f\"  - Presets: {presets}\")\n",
    "    print(f\"  - Bag folds: {num_bag_folds}\")\n",
    "    print(f\"  - Stack levels: {num_stack_levels}\")\n",
    "    \n",
    "    # Configure AutoGluon\n",
    "    predictor = TabularPredictor(\n",
    "        label=target_column,\n",
    "        problem_type='binary',\n",
    "        eval_metric=eval_metric,\n",
    "        path='./autogluon_models'\n",
    "    )\n",
    "    \n",
    "    # Train models\n",
    "    predictor.fit(\n",
    "        train_data=train_data,\n",
    "        time_limit=time_limit,\n",
    "        presets=presets,\n",
    "        num_bag_folds=num_bag_folds,\n",
    "        num_stack_levels=num_stack_levels,\n",
    "        verbosity=2\n",
    "    )\n",
    "    \n",
    "    print(\"\\n✓ AutoGluon training complete!\")\n",
    "    return predictor\n",
    "\n",
    "# Train models\n",
    "predictor = train_autogluon_models(\n",
    "    train_data=train_df,\n",
    "    target_column=CONFIG['target_column'],\n",
    "    time_limit=CONFIG['time_limit'],\n",
    "    presets=CONFIG['presets'],\n",
    "    eval_metric=CONFIG['eval_metric'],\n",
    "    num_bag_folds=CONFIG['num_bag_folds'],\n",
    "    num_stack_levels=CONFIG['num_stack_levels']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Evaluate Models and Select Best\n",
    "\n",
    "Evaluate all trained models and select the best based on F1 Score, Recall, and Precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_and_select_best_model(\n",
    "    predictor: TabularPredictor,\n",
    "    test_data: pd.DataFrame,\n",
    "    target_column: str\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Evaluate all trained models and select the best one.\n",
    "    \"\"\"\n",
    "    print(\"Evaluating models...\")\n",
    "    \n",
    "    # Get leaderboard\n",
    "    leaderboard = predictor.leaderboard(data=test_data, silent=True)\n",
    "    \n",
    "    # Get detailed metrics for each model\n",
    "    model_metrics = []\n",
    "    \n",
    "    for model_name in leaderboard['model']:\n",
    "        try:\n",
    "            # Get predictions\n",
    "            y_true = test_data[target_column]\n",
    "            y_pred = predictor.predict(test_data, model=model_name)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "            recall = recall_score(y_true, y_pred, average='weighted')\n",
    "            precision = precision_score(y_true, y_pred, average='weighted')\n",
    "            accuracy = accuracy_score(y_true, y_pred)\n",
    "            \n",
    "            model_metrics.append({\n",
    "                'model_name': model_name,\n",
    "                'f1_score': f1,\n",
    "                'recall': recall,\n",
    "                'precision': precision,\n",
    "                'accuracy': accuracy,\n",
    "                'score_val': leaderboard[leaderboard['model'] == model_name]['score_val'].values[0]\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not evaluate {model_name}: {e}\")\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    metrics_df = pd.DataFrame(model_metrics)\n",
    "    \n",
    "    # Select best model based on F1, then Recall, then Precision\n",
    "    metrics_df = metrics_df.sort_values(\n",
    "        by=['f1_score', 'recall', 'precision'],\n",
    "        ascending=False\n",
    "    )\n",
    "    \n",
    "    best_model = metrics_df.iloc[0].to_dict()\n",
    "    \n",
    "    return {\n",
    "        'best_model': best_model,\n",
    "        'all_models': metrics_df.to_dict('records'),\n",
    "        'leaderboard': leaderboard\n",
    "    }\n",
    "\n",
    "# Evaluate models\n",
    "evaluation_results = evaluate_and_select_best_model(\n",
    "    predictor=predictor,\n",
    "    test_data=test_df,\n",
    "    target_column=CONFIG['target_column']\n",
    ")\n",
    "\n",
    "# Display results\n",
    "best_model = evaluation_results['best_model']\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"BEST MODEL SELECTED: {best_model['model_name']}\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"F1 Score:  {best_model['f1_score']:.4f}\")\n",
    "print(f\"Recall:    {best_model['recall']:.4f}\")\n",
    "print(f\"Precision: {best_model['precision']:.4f}\")\n",
    "print(f\"Accuracy:  {best_model['accuracy']:.4f}\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Show all models\n",
    "print(f\"\\nAll Models Performance:\")\n",
    "metrics_df = pd.DataFrame(evaluation_results['all_models'])\n",
    "print(metrics_df[['model_name', 'f1_score', 'recall', 'precision', 'accuracy']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Register Model to Model Registry\n",
    "\n",
    "Register the best model to Snowflake Model Registry with metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def register_model_to_warehouse(\n",
    "    predictor: TabularPredictor,\n",
    "    model_name: str,\n",
    "    best_model_name: str,\n",
    "    metrics: Dict,\n",
    "    feature_list: List[str],\n",
    "    target_column: str\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Register the best model to Snowflake Model Registry.\n",
    "    \"\"\"\n",
    "    print(f\"Registering model '{model_name}' to Snowflake Model Registry...\")\n",
    "    \n",
    "    # Prepare model metadata\n",
    "    metadata = {\n",
    "        'model_type': 'AutoGluon TabularPredictor',\n",
    "        'best_model': best_model_name,\n",
    "        'f1_score': metrics['best_model']['f1_score'],\n",
    "        'recall': metrics['best_model']['recall'],\n",
    "        'precision': metrics['best_model']['precision'],\n",
    "        'accuracy': metrics['best_model']['accuracy'],\n",
    "        'features': feature_list,\n",
    "        'target_column': target_column,\n",
    "        'training_timestamp': datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    # Register model\n",
    "    model_version = registry.log_model(\n",
    "        model_name=model_name,\n",
    "        model_version='v1',\n",
    "        model=predictor,\n",
    "        conda_dependencies=['autogluon.tabular'],\n",
    "        metadata=metadata,\n",
    "        comment=f\"Best model: {best_model_name} with F1={metrics['best_model']['f1_score']:.4f}\"\n",
    "    )\n",
    "    \n",
    "    return model_version\n",
    "\n",
    "# Register model\n",
    "model_version = register_model_to_warehouse(\n",
    "    predictor=predictor,\n",
    "    model_name=CONFIG['model_name'],\n",
    "    best_model_name=evaluation_results['best_model']['model_name'],\n",
    "    metrics=evaluation_results,\n",
    "    feature_list=selected_features,\n",
    "    target_column=CONFIG['target_column']\n",
    ")\n",
    "\n",
    "print(f\"✓ Model registered successfully: {model_version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Workflow Results Summary\n",
    "\n",
    "Display complete workflow results and metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile final results\n",
    "final_results = {\n",
    "    'workflow_status': 'SUCCESS',\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'data_info': {\n",
    "        'source_table': CONFIG['source_table'],\n",
    "        'total_rows': len(training_data),\n",
    "        'train_rows': len(train_df),\n",
    "        'test_rows': len(test_df)\n",
    "    },\n",
    "    'feature_selection': {\n",
    "        'total_features': correlation_report['total_numeric_features'],\n",
    "        'selected_features': correlation_report['selected_features'],\n",
    "        'dropped_features': correlation_report['dropped_features'],\n",
    "        'correlation_threshold': correlation_report['correlation_threshold']\n",
    "    },\n",
    "    'feature_view': CONFIG['feature_view_name'],\n",
    "    'model_info': {\n",
    "        'model_name': CONFIG['model_name'],\n",
    "        'model_version': model_version,\n",
    "        'best_model': evaluation_results['best_model']['model_name']\n",
    "    },\n",
    "    'performance_metrics': {\n",
    "        'f1_score': evaluation_results['best_model']['f1_score'],\n",
    "        'recall': evaluation_results['best_model']['recall'],\n",
    "        'precision': evaluation_results['best_model']['precision'],\n",
    "        'accuracy': evaluation_results['best_model']['accuracy']\n",
    "    },\n",
    "    'models_evaluated': len(evaluation_results['all_models'])\n",
    "}\n",
    "\n",
    "# Display results\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"WORKFLOW COMPLETED SUCCESSFULLY\")\n",
    "print(\"=\"*80)\n",
    "print(json.dumps(final_results, indent=2))\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Optional: Deploy to Container Services\n",
    "\n",
    "Deploy the model to Snowflake Container Services for production inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CONFIG['deploy_to_container']:\n",
    "    def deploy_to_container_services(\n",
    "        model_name: str,\n",
    "        model_version: str,\n",
    "        compute_pool: str,\n",
    "        service_name: str\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        Deploy model to Snowflake Container Services.\n",
    "        \"\"\"\n",
    "        print(f\"Deploying model to Container Services: {service_name}\")\n",
    "        \n",
    "        # Create service specification\n",
    "        service_spec = f\"\"\"\n",
    "        CREATE SERVICE IF NOT EXISTS {service_name}\n",
    "        IN COMPUTE POOL {compute_pool}\n",
    "        FROM @ML_MODELS.MODEL_REGISTRY.{model_name}/{model_version}\n",
    "        SPECIFICATION = $$\n",
    "        spec:\n",
    "          containers:\n",
    "          - name: model-inference\n",
    "            image: /ml_models/model_registry/autogluon:latest\n",
    "            env:\n",
    "              MODEL_NAME: {model_name}\n",
    "              MODEL_VERSION: {model_version}\n",
    "          endpoints:\n",
    "          - name: predict\n",
    "            port: 8080\n",
    "            protocol: http\n",
    "        $$\n",
    "        \"\"\"\n",
    "        \n",
    "        # Execute service creation\n",
    "        session.sql(service_spec).collect()\n",
    "        \n",
    "        endpoint = f\"https://{service_name}.snowflakecomputing.app/predict\"\n",
    "        print(f\"✓ Service deployed successfully!\")\n",
    "        print(f\"✓ Endpoint: {endpoint}\")\n",
    "        \n",
    "        return endpoint\n",
    "    \n",
    "    # Deploy model\n",
    "    service_endpoint = deploy_to_container_services(\n",
    "        model_name=CONFIG['model_name'],\n",
    "        model_version=model_version,\n",
    "        compute_pool=CONFIG['compute_pool'],\n",
    "        service_name=CONFIG['service_name']\n",
    "    )\n",
    "    \n",
    "    final_results['service_endpoint'] = service_endpoint\n",
    "else:\n",
    "    print(\"Container Services deployment is disabled in configuration.\")\n",
    "    print(\"Set CONFIG['deploy_to_container'] = True to enable deployment.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Make Predictions (Optional)\n",
    "\n",
    "Use the trained model to make predictions on new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on test set (sample)\n",
    "sample_data = test_df.head(10).drop(columns=[CONFIG['target_column']])\n",
    "\n",
    "predictions = predictor.predict(sample_data)\n",
    "prediction_probs = predictor.predict_proba(sample_data)\n",
    "\n",
    "# Display predictions\n",
    "results_df = sample_data.copy()\n",
    "results_df['Prediction'] = predictions\n",
    "results_df['Probability_0'] = prediction_probs[0]\n",
    "results_df['Probability_1'] = prediction_probs[1]\n",
    "\n",
    "print(\"Sample Predictions:\")\n",
    "print(results_df[[CONFIG['entity_column'], 'Prediction', 'Probability_0', 'Probability_1']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrated a complete end-to-end ML workflow in Snowflake:\n",
    "\n",
    "1. **Data Loading**: Loaded data from Snowflake table\n",
    "2. **Feature Selection**: Automated correlation-based feature selection\n",
    "3. **Feature Store**: Registered features for versioning and reuse\n",
    "4. **Model Training**: Trained multiple models with AutoGluon\n",
    "5. **Model Evaluation**: Selected best model based on metrics\n",
    "6. **Model Registry**: Registered model with metadata for tracking\n",
    "7. **Deployment**: Optional deployment to Container Services\n",
    "\n",
    "### Next Steps:\n",
    "- Monitor model performance over time\n",
    "- Retrain model with new data periodically\n",
    "- Experiment with different feature engineering techniques\n",
    "- Tune AutoGluon hyperparameters for better performance\n",
    "- Set up automated retraining pipelines\n",
    "\n",
    "### Resources:\n",
    "- [Snowflake ML Documentation](https://docs.snowflake.com/en/developer-guide/snowpark-ml/index)\n",
    "- [AutoGluon Documentation](https://auto.gluon.ai/)\n",
    "- [GitHub Repository](https://github.com/randalscottking/snowflake-autogluon-classification)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
